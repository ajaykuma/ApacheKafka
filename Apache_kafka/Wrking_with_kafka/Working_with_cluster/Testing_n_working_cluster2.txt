--change hostnames accordingly
TO BE VERIFIED..
#working with kafka cluster

#all datasets used here can be found in 'https://github.com/ajaykuma/datasets_for_work' or

use wget
wget https://github.com/ajaykuma/Datasets_For_Work/blob/main/Bank_full.csv
--create topic BankData with specific partitions and repl factor
[hdu@ch1 ~]$ kafka-topics.sh --create --topic BankData --partitions 3 --replication-factor 3 --bootstrap-server ch1:9092
[hdu@ch1 ~]$ kafka-console-producer.sh --topic BankData --broker-list ch1:9092 < <path>/Bank_full.csv

kafka-run-class.sh kafka.tools.GetOffsetShell --topic BankData --broker-list ch1:9092

--without creating topic explicitly
kafka-console-producer.sh --topic BankData1 --broker-list ch1:9092 < <path>/Bank_full.csv

kafka-run-class.sh kafka.tools.GetOffsetShell --topic BankData1 --broker-list ch1:9092

--optional---
kafka-console-producer.sh --topic Test1 --broker-list ch1:9092 < <path>/global_superstore_2016_v.csv
--optional---

--optional---
kafka-topics.sh --create --topic PrimerData --partitions 3 --replication-factor 3 --bootstrap-server ch1:9092
kafka-console-producer.sh --topic PrimerData --broker-list ch1:9092 < /home/hdu/Downloads/primer-dataset.json
kafka-console-consumer.sh --topic PrimerData --bootstrap-server ch1:9092 --from-beginning
--optional---

kafka-topics.sh --create --topic bankdata2 --bootstrap-server ch1:9092 --partitions 3 --replication-factor 3

--edit config
kafka-configs.sh --bootstrap-server ch1:9092 --alter --entity-type topics --entity-name bankdata2 
--add-config segment.bytes=2000000

--check config
kafka-configs.sh --describe --all --topic bankdata2 --bootstrap-server c1:9092 | grep segment

--push some data
kafka-console-producer.sh --broker-list c1:9092 --topic bankdata2 < Downloads/Bank_full.csv
kafka-console-producer.sh --broker-list c1:9092 --topic bankdata2 < Downloads/Bank_full.csv

--look at offsets
kafka-run-class.sh kafka.tools.GetOffsetShell --topic bankdata2 --broker-list c1:9092

--look at .log files
ls -all /usr/local/kafka/kafka-logs/bankdata2-0/
ls -all /usr/local/kafka/kafka-logs/bankdata2-1/
ls -all /usr/local/kafka/kafka-logs/bankdata2-2/

kafka-dump-log.sh --files /usr/local/kafka/kafka-logs/bankdata2-0/00000000000000000000.log
kafka-dump-log.sh --files /usr/local/kafka/kafka-logs/bankdata2-0/00000000000000000000.timeindex
kafka-dump-log.sh --files /usr/local/kafka/kafka-logs/bankdata2-0/00000000000000000000.index

#other option specifying serializer
kafka-console-producer.sh --topic primerdata --broker-list localhost:9092 --property 
value-serializer="import org.apache.kafka.connect.json.JsonSerializer" < employees.json

kafka-console-consumer.sh --topic primerdata --bootstrap-server localhost:9092 --property value-serializer="org.springframework.kafka.support.serializer.JsonDeserializer" --from-beginning
{"name":"Michael", "salary":3000}


Other commands/options
#look at configs which can be changed
kafka-configs.sh --zookeeper c1:2181

#Understanding Logs

#looking to timeindex log
C:\Users\Wic10\Downloads\kafka\kafka_2.11-2.2.1\bin\windows>kafka-dump-log.bat --files C:\Users\Wic10\Downloads\Java\Kafka_Prod_Consu_for_string2\Kafka_Project\tmp\kafka-logs\Test1-0\00000000000000000000.timeindex

#performance test
kafka-producer-perf-test.sh --topic Test2 --record-size 1024 --throughput -1 --num-records 1000000 --producer-props acks=all bootstrap.servers=c1:9092

kafka-run-class.sh kafka.tools.EndToEndLatency c1:9092,c2:9092,c3:9092 Test2 10000 1 10000

kafka-consumer-perm-test.sh --topic Test2 --broker-list c1:9092 --messages 1000000

#looking into metadata nodes
kafka-metadata-shell.sh --snapshot /usr/local/kafka/kafka-logs/bankdata-0/00000000000000000030.snapshot

#Connecting to zookeeper
[hdu@c1 Working_with_cluster]$ zookeeper-shell.sh c1:2181
Connecting to c1:2181
Welcome to ZooKeeper!
JLine support is disabled

WATCHER::

WatchedEvent state:SyncConnected type:None path:null
ls
ls [-s] [-w] [-R] path
ls /
[admin, brokers, cluster, config, consumers, controller, controller_epoch, feature, isr_change_notification, latest_producer_id_block, log_dir_event_notification, zookeeper]
ls /cluster
[id]
ls /cluster/id
[]
get /cluster/id
{"version":"1","id":"Sk_5hgxbTCSEjlYj8_KZtA"}
ls /config
[brokers, changes, clients, ips, topics, users]
ls /config/topics
[Topic1, Topic2, Topic3, __consumer_offsets, bankdata, employees, employees1, hello_world_topic, primerdata]
get /config/topics/Topic1
{"version":1,"config":{}}
ls /config/topics/Topic1

quit

#Reassignment:
kafka-topics.sh --describe --topic primerdata --bootstrap-server c1:9092
Topic: primerdata	TopicId: rbuXpXn-QeKcH5DlHqs4AQ	PartitionCount: 3	ReplicationFactor: 3	Configs: segment.bytes=2000000
	Topic: primerdata	Partition: 0	Leader: 1	Replicas: 1,0,2	Isr: 0,2,1
	Topic: primerdata	Partition: 1	Leader: 0	Replicas: 0,2,1	Isr: 0,2,1
	Topic: primerdata	Partition: 2	Leader: 2	Replicas: 2,1,0	Isr: 0,2,1

--create json file 'primerdata-test.json
{"topics":
    [{"topic": "primerdata"}],
     "version":1
      }


[hdu@c1 ~]$ kafka-reassign-partitions.sh --bootstrap-server c1:9092 --topics-to-move-json-file primerdata-test.json --broker-list "2,3,1" --generate
Current partition replica assignment
{"version":1,"partitions":[{"topic":"primerdata","partition":0,"replicas":[1,0,2],"log_dirs":["any","any","any"]},{"topic":"primerdata","partition":1,"replicas":[0,2,1],"log_dirs":["any","any","any"]},{"topic":"primerdata","partition":2,"replicas":[2,1,0],"log_dirs":["any","any","any"]}]}

Proposed partition reassignment configuration
{"version":1,"partitions":[{"topic":"primerdata","partition":0,"replicas":[1,2,3],"log_dirs":["any","any","any"]},{"topic":"primerdata","partition":1,"replicas":[2,3,1],"log_dirs":["any","any","any"]},{"topic":"primerdata","partition":2,"replicas":[3,1,2],"log_dirs":["any","any","any"]}]}


--save proposed partition into a file
[hdu@c1 ~]$ kafka-reassign-partitions.sh --bootstrap-server c1:9092 --reassignment-json-file expand-cluster-reassignment.json --execute 
Current partition replica assignment

{"version":1,"partitions":[{"topic":"primerdata","partition":0,"replicas":[1,0,2],"log_dirs":["any","any","any"]},{"topic":"primerdata","partition":1,"replicas":[0,2,1],"log_dirs":["any","any","any"]},{"topic":"primerdata","partition":2,"replicas":[2,1,0],"log_dirs":["any","any","any"]}]}

Save this to use as the --reassignment-json-file option during rollback
Successfully started partition reassignments for primerdata-0,primerdata-1,primerdata-2

--verify
