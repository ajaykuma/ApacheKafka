Jconsole < JMX
https://gist.github.com/thbkrkr/77c8f6f9a301d7b16555726793af8301
https://access.redhat.com/documentation/en-us/red_hat_amq/7.2/html/using_amq_streams_on_red_hat_enterprise_linux_rhel/monitoring-str

For Windows
#if Java already installed on windows
#if Local kafka setup on windows

#First:
to designate a port that JConsole can use to collect JMX metrics from your Kafka host. 
Edit Kafka’s startup script—bin/kafka-run-class.sh
 —to include the value of the JMX port by adding the following parameters to the KAFKA_JMX_OPTS variable:
For example:

-Dcom.sun.management.jmxremote.port=9999 -Dcom.sun.management.jmxremote.rmi.port=9999 -Djava.rmi.server.hostname=127.0.0.1

>start ur kafka cluster
>start JCONSOLE by typing in JCONSOLE

explore Jconsole > (especially Mbeans TAB)
--useful information regarding MBEANs is provided above.

For linux:
--Assuming your kafka cluster is setup on multiple linux machines
Say Centos:

$echo $DISPLAY
--see if it shows something
$jconsole
--if it doesn't work
$sudo yum install -y xterm
$xterm &
$sudo yum install -y xorg-x11-xauth
$sudo vi /etc/ssh/sshd_config
AllowTcpForwarding yes
ForwardX11 yes
X11Forwarding yes

$reboot
$jconsole
--if kafka is running, check under local process and connect to kafka process
--to be able to connect to kafka processes running on other machines, make similar changes in 
kafka_path/bin/kafka-run-class.sh
 
=================
Other options-1:
Burrow:
Burrow is a specialized monitoring tool developed by LinkedIn specifically for Kafka consumer monitoring. Burrow gives you visibility into Kafka’s offsets, topics, and consumers.

By consuming the special internal Kafka topic __consumer_offsets, Burrow can act as a centralized service, 
separate from any single consumer, giving you an objective view of consumers based on 
both their committed offsets (across topics) and broker state

Install and configure: GO
https://golang.org/doc/install

#to Install Burrow:
    go get github.com/linkedin/burrow
    cd $GOPATH/src/github.com/linkedin/burrow
    go mod tidy
    go install

Before you can use Burrow, you’ll need to write a configuration file.
Your Burrow configuration will vary depending on your Kafka deployment.

Sample file:
[zookeeper]
servers=["localhost:2181" ]

[httpserver.mylistener]
address=":8080"

[cluster.local]
class-name="kafka"
servers=[ "localhost:9091", "localhost:9092", "localhost:9093" ]

[consumer.myconsumers]
class-name="kafka"
cluster="local"
servers=[ "localhost:9091", "localhost:9092", "localhost:9093" ]
offsets-topic="__consumer_offsets"

Burrow documentation:
https://github.com/linkedin/Burrow/wiki/Configuration

Once Burrow configured:
we can begin tracking consumer health by running this command:

$GOPATH/bin/burrow --config-dir /path/to/config-directory

We can begin querying Burrow’s HTTP endpoints. For example, to see a list of our Kafka clusters, we can 
hit http://localhost:8080/v3/kafka and see a JSON response like the one shown here:

{
	"error": false,
	"message": "cluster list returned",
	"clusters": ["local"],
	"request": {
		"url": "/v3/kafka",
		"host": "mykafkahost"
	}
}

#Monitor Kafka’s page cache
download the cachestat script created by Brendan Gregg

wget https://raw.githubusercontent.com/brendangregg/perf-tools/master/fs/cachestat

chmod +x cachestat

./cachestat <collection interval in seconds>

Sample output:
Counting cache functions... Output every 20 seconds.
	    HITS   MISSES  DIRTIES    RATIO   BUFFERS_MB   CACHE_MB
	    5352        0      234   100.0%          103        165
	    5168        0      260   100.0%          103        165
	    6572        0      259   100.0%          103        165
	    6504        0      253   100.0%          103        165
	[...]

The values in the DIRTIES column show the number of pages that have been modified after entering the page cache.

Other options-2:
https://docs.itrsgroup.com/docs/geneos/5.3.0/Integrations/Kafka/kafka_monitoring_ug.html

Option options-3:
Setup confluent-center

https://www.confluent.io/get-started/?product=software

Provide your email and choose 'free'
choose option: zip
unzip and set environment variable to confluent path
export CONFLUENT_HOME=<path-to-confluent>

Install the Kafka Connect Datagen source connector using the Confluent Hub client. This connector generates mock data
confluent-hub install \
   --no-prompt confluentinc/kafka-connect-datagen:latest

confluent local services start

Control Center web interface at http://localhost:9021

For all details:
https://docs.confluent.io/platform/current/quickstart/ce-quickstart.html

