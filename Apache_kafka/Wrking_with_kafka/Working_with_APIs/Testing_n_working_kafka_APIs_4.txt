#Setting up Producer and Consumer with custom properties.
##Create a topic with 3 repl and 3 partitions (provided you have a 3 node cluster)
kafka-topics.sh --create --topic hello_world_topic --partitions 3 --replication-factor 3 --bootstrap-server c1:9092
kafka-topics.sh --list --bootstrap-server c1:9092
kafka-topics.sh --describe --topic hello_world_topic --bootstrap-server c1:9092

##Create a producer sample application
BasicProducer.java
----
package clients;
import java.util.Properties;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerRecord;

public class BasicProducer {
     public static void main(String[] args) {
	System.out.println("***Starting Basic Producer***");

        //configuration
        Properties settings = new Properties();
        settings.put("client.id", "basic-producer-v0.1.0");
        settings.put("bootstrap.servers", "c1:9092,c2:9093,c3:9092");
        settings.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        settings.put("value.serializer","org.apache.kafka.common.serialization.StringSerializer");

        //create producer
        final KafkaProducer<String,String> producer = new KafkaProducer<>(settings);

       //shutdown behaviour
       Runtime.getRuntime().addShutdownHook(new Thread(() -> {
	   System.out.println("###Stopping Basic Producer###");
	   producer.close();
       }));

       final String topic = "hello_world_topic";
       for(int i=1; i<=5; i++){
          final String key = "key-" + i;
          final String value = "value-" + i;
          final ProducerRecord<String, String> record = new ProducerRecord<>(topic, key, value);
	  producer.send(record);
       }
     }
}
----
--run the BasicProducer
--test if topic was created
--check messages from topic using console consumer
kafka-console-consumer.sh --topic hello_world_topic --bootstrap-server c1:9092 --from-beginning --property print.key=true
--delete the topic
kafka-topics.sh --delete --topic hello_world_topic --bootstrap-server c1:9092
--create topic again
kafka-topics.sh --create --topic hello_world_topic --partitions 3 --replication-factor 3 --bootstrap-server c1:9092

##Create a consumer sample application
BasicConsumer.java
----
package clients;

import org.apache.kafka.clients.consumer.*;
import org.apache.kafka.clients.consumer.Consumer;
import org.apache.kafka.common.serialization.StringDeserializer;
//import org.apache.kafka.common.serialization.StringDeserializer;

import java.time.Duration;
import java.util.Collections;
import java.util.Properties;

public class BasicConsumer {
	public static void main(String... args) throws Exception {
	      runConsumer();
	  }

    private final static String TOPIC = "hello_world_topic";
    private final static String BOOTSTRAP_SERVERS =
            "c1:9092,c2:9092,c3:9092";
    
    private static Consumer<String, String> createConsumer() {
        final Properties props = new Properties();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,BOOTSTRAP_SERVERS);
        props.put(ConsumerConfig.GROUP_ID_CONFIG,"BasicConsumer1");
        //props.put(ConsumerConfig.GROUP_ID_CONFIG,"BasicConsumer" + System.currentTimeMillis());
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,StringDeserializer.class.getName());
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,StringDeserializer.class.getName());
        //props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,"earliest");

        // Create the consumer using props.
        final Consumer<String, String> consumer =
                                    new KafkaConsumer<>(props);

        // Subscribe to the topic.
        consumer.subscribe(Collections.singletonList(TOPIC));
        return consumer;
    }
    static void runConsumer() throws InterruptedException {
        final Consumer<String, String> consumer = createConsumer();

        final int giveUp = 100;   int noRecordsCount = 0;

        while (true) {
            final ConsumerRecords<String, String> consumerRecords =
                    consumer.poll(Duration.ofMillis(1000));

            if (consumerRecords.count()==0) {
                noRecordsCount++;
                if (noRecordsCount > giveUp) break;
                else continue;
            }

            consumerRecords.forEach(record -> {
                System.out.printf("Consumer Record:(%s, %s, %d, %d)\n",
                        record.key(), record.value(),
                        record.partition(), record.offset());
            });

            consumer.commitAsync();
        }
        consumer.close();
        System.out.println("DONE");
    }
  }

----

--Scenario1: if groupID remains same and multiple instances of same appl running
            no auto_offset set to "earliest"
so among other properties,
props.put(ConsumerConfig.GROUP_ID_CONFIG,"BasicConsumer1");
//props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,"earliest");

--run 3 instances of same application
--run producer application
--look into console as how data is being consumed by appls
Thus, we can see each application was assigned a partition and consumed messages from that specific partition only.

--Scenario2: set groupID different for each instance of application
so among properties,
//props.put(ConsumerConfig.GROUP_ID_CONFIG,"BasicConsumer1");
//props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,"earliest");
props.put(ConsumerConfig.GROUP_ID_CONFIG,"BasicConsumer" + System.currentTimeMillis());

Thus, we can see each application accesses data from each partition in round-robin fashion & all records.

--Scenario3: change auto_offset to earliest and run all applications with different groupID
Look at records which were consumed and each will consume from beginning..
Try adding more and new data to topic and check if each instance of appl consumes from beginning..

--Scenario4: While running multiple instances of customer application, (each having same groupID)
  >kill once instance and check if autobalancing or distribution of consuming happens to remaining instances
  > add a new instance and check if this remains empty or is assigned some partition

#Note**The Kafka producer api does not allow you to create custom partition, if you try to produce some data to a topic 
which does not exists it will first create the topic if the auto.create.topics.enable property in the BrokerConfig 
is set to TRUE and start publishing data on the same but the number of partitions created for this topic 
will based on the num.partitions parameter defined in the configuration files (by default it is set to 1).
Increasing partition count for an existing topic can be done, but it'll not move any existing data into those partitions.







	





