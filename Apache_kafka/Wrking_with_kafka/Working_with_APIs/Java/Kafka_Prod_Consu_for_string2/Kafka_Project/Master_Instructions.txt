#Creating all in one project
--copy the Kafka_Prod_Consu_for_string2 project from git onto your local machine
--open this project from filesystem within your IDE
--refer scripts folder &
--run standalone cluster say on windows/linux machines where
  --zookeeper.properties will point to a path for zookeeper which resides in our project folder
  example: dataDir=C:\\Users\\Win10\\Downloads\\Java\\Kafka_Prod_Consu_for_string2\\Kafka_Project\\tmp\\zookeeper
  --server.properties will point to a path for kafka which resides in our project folder
  example: log.dirs=C:\\Users\\Win10\\Downloads\\Java\\Kafka_Prod_Consu_for_string2\\Kafka_Project\\tmp\\kafka-logs
--refresh project to see new folders created for 'zookeeper' and 'kafka'

--When broker starts some files are created within kafka folder, i.e. in our case ...\\tmp\\kafka-logs

cleaner-offset-checkpoint
log-start-offset-checkpoint
meta.properties
recovery-point-offset-checkpoint
replication-offset-checkpoint

--these files would be empty as of now

--For each partition of topic we have additional files
xxxxx.index
xxxxx.log
xxxxx.timeindex
leader-epoch-checkpoint

xxxxx.log ---> first log segment file

--push some data into this index
--refresh to check if data was pushed into topic

--as partition starts receiving data/messages, these are stored in 'segment-0' or first segment file until
--the segment limit is reached. Once limit is reached the segment file is closed and new segment is created
--max segement size : 1 week data /1 gb 
--Each kafka message is uniquely identified by 64 bit integer offset
--Numbering of offset continues across segments
segment0-msg1:M-0000
segment0-msg2:M-0001
segment0-msg3:M-0002
....
segment0-msg30652:M-30652
segment1-msg30651:M-30653
...
--This ensures the offset is unique within the partition
--Note** Segment file name is suffixed by the first offset in that segment.

--To locate messages we need Topic name + partition number + offset number
--in Stream processing application,messages are read in sequence per partition

--To help brokers find the message at an offset, kafka maintains an index of offset (xxxx.index)
--the index files are also segmented, stored with log files per partition

--when seeking messages based on timestamp, kafka maintains a timestamp for each message and builds a timeindex.
--the timeindex files are also segmented, stored with log files per partition










