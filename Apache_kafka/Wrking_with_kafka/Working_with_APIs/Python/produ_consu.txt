#working with python api
--check python
--if python3 installed, set alias
alias python=python3

sudo apt install python3-pip
pip3 install kafka-python

#create topic
kafka-topics.sh --create --zookeeper m3:2181 --replication-factor 1 --partitions 1 --topic numtest


#create a producer py file

bootstrap_servers=[‘localhost:9092’]: sets the host and port the producer should contact to bootstrap initial cluster metadata. It is not necessary to set this here, since the default is localhost:9092.

value_serializer=lambda x: dumps(x).encode(‘utf-8’): function of how the data should be serialized before sending to the broker. Here, we convert the data to a json file and encode it to utf-8.

we want to generate numbers from one till 1000. This can be done with a for-loop where we feed each number as the value into a dictionary with one key: number. This is not the topic key, but just a key of our data. Within the same loop we will also send our data to a broker

our value serializer will automatically convert and encode the data

--refer producer.py file

#create a consumer py file to read from topic
The first argument is the topic, numtest in our case.
bootstrap_servers=[‘localhost:9092’]: same as our producer
auto_offset_reset=’earliest’: one of the most important arguments. It handles where the consumer restarts reading after breaking down or being turned off and can be set either to earliest or latest. When set to latest, the consumer starts reading at the end of the log. When set to earliest, the consumer starts reading at the latest committed offset.
enable_auto_commit=True: makes sure the consumer commits its read offset every interval.
auto_commit_interval_ms=1000ms: sets the interval between two commits. Since messages are coming in every five second, committing every second seems fair.
group_id=’counters’: this is the consumer group to which the consumer belongs. Remember from the introduction that a consumer needs to be part of a consumer group to make the auto commit work.
The value deserializer deserializes the data into a common json format, the inverse of what our value serializer was doing.

We can extract the data from our consumer by looping through it (the consumer is an iterable). The consumer will keep listening until the broker doesn’t respond anymore. A value of a message can be accessed with the value attribute. Here, we overwrite the message with the message value.

--refer consumer.py file

--run producer.py
<terminal 1> python producer.py
<terminal 2> python consumer.py
<terminal 3> optionally use command line 'kafka-console-consumer' to read data

-------------------------
Additional
--when you want the data to be read by consumer and stored in a database say (mongodb)
--needs mongodb sngle instance installed

--refer consumer2.py

--connecting to a numtest collection in mongodb
client = MongoClient('localhost:27017')
collection = client.numtest.numtest

--We can extract the data from our consumer by looping through it (the consumer is an iterable). The consumer will keep listening until the broker doesn’t respond anymore. A value of a message can be accessed with the value attribute. Here, we overwrite the message with the message value.
Then insert the data into our database collection. The last line prints a confirmation that the message was added to our collection.

for message in consumer:
    message = message.value
    collection.insert_one(message)
    print('{} added to {}'.format(message, collection))

